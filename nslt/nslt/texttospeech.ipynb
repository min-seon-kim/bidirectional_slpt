{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "class Target:\n",
    "    watchDir = os.getcwd()\n",
    "    #watchDir에 감시하려는 디렉토리를 명시한다.\n",
    "    def __init__(self):\n",
    "        self.observer = Observer()   #observer객체를 만듦\n",
    "    def run(self):\n",
    "        event_handler = Handler()\n",
    "        self.observer.schedule(event_handler, self.watchDir, recursive=True)\n",
    "        self.observer.start()\n",
    "        try:\n",
    "            while True:\n",
    "                time.sleep(1)\n",
    "        except:\n",
    "            self.observer.stop()\n",
    "            print(\"Error\")\n",
    "            self.observer.join()\n",
    "            \n",
    "class Handler(FileSystemEventHandler):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ckpt = tf.train.latest_checkpoint(\"../Output\")\n",
    "#FileSystemEventHandler 클래스를 상속받음.\n",
    "#아래 핸들러들을 오버라이드 함\n",
    "    #파일, 디렉터리가 move 되거나 rename 되면 실행\n",
    "#     def on_moved(self, event):\n",
    "#         print(event)\n",
    "#     def on_created(self, event): #파일, 디렉터리가 생성되면 실행\n",
    "#         print(event)\n",
    "#     def on_deleted(self, event): #파일, 디렉터리가 삭제되면 실행\n",
    "#         print(event)\n",
    "    def on_modified(self, event): #파일, 디렉터리가 수정되면 실행\n",
    "        # Inference\n",
    "        inference_fn(self.ckpt, '../Data/test.sign' './predictions.de', hparams, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"TensorFlow NMT model implementation.\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import inference\n",
    "import train\n",
    "from utils import evaluation_utils\n",
    "from utils import misc_utils as utils\n",
    "from utils import vocab_utils\n",
    "\n",
    "# utils.check_tensorflow_version()\n",
    "global hparams\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "def add_arguments(parser):\n",
    "    \"\"\"Build ArgumentParser.\"\"\"\n",
    "    parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
    "\n",
    "    # network\n",
    "    parser.add_argument(\"--num_units\",                  type=int,       default=32,     help=\"Network size.\")\n",
    "    parser.add_argument(\"--num_layers\",                 type=int,       default=2,      help=\"Network depth.\")\n",
    "    parser.add_argument(\"--encoder_type\",               type=str,       default=\"uni\",  help=\"\"\"\\ uni | bi | gnmt. For bi, we build num_layers/2 bi-directional layers.For gnmt, we build 1 bi-directional layer, and (num_layers - 1) unidirectional layers. \"\"\")\n",
    "    parser.add_argument(\"--num_embeddings_partitions\",  type=int,       default=0,      help=\"Number of partitions for embedding vars.\")\n",
    "    parser.add_argument(\"--residual\",                   type=\"bool\",    default=False,  nargs=\"?\", const=True,  help=\"Whether to add residual connections.\")\n",
    "    parser.add_argument(\"--time_major\",                 type=\"bool\",    default=True,   nargs=\"?\", const=True,  help=\"Whether to use time-major mode for dynamic RNN.\")\n",
    "\n",
    "    # attention mechanisms\n",
    "    parser.add_argument(\"--attention\",              type=str,       default=\"\",         help=\"\"\"\\luong | scaled_luong | bahdanau | normed_bahdanau or set to \"\" for no attention\\ \"\"\")\n",
    "    parser.add_argument(\"--pass_hidden_state\",      type=\"bool\",    default=True,       nargs=\"?\", const=True,  help=\"\"\"\\ Whether to pass encoder's hidden state to decoder when using an attention based model.\\  \"\"\")\n",
    "    parser.add_argument(\"--attention_architecture\", type=str,       default=\"standard\", help=\"\"\"\\ standard | gnmt | gnmt_v2. standard: use top layer to compute attention. \n",
    "                                                                                                                          gnmt: GNMT style of computing attention, use previous bottom layer to compute attention.\n",
    "                                                                                                                          gnmt_v2: similar to gnmt, but use current bottom layer to compute attention.\\ \"\"\")\n",
    "\n",
    "    # optimizer\n",
    "    parser.add_argument(\"--optimizer\",                      type=str,       default=\"adam\", help=\"sgd | adam\")\n",
    "    parser.add_argument(\"--learning_rate\",                  type=float,     default=0.00001, help=\"Learning rate. Adam: 0.001 | 0.0001\")\n",
    "    parser.add_argument(\"--start_decay_step\",               type=int,       default=0,      help=\"When we start to decay\")\n",
    "    parser.add_argument(\"--decay_steps\",                    type=int,       default=10000,  help=\"Learning Rate-How frequent we decay\")\n",
    "    parser.add_argument(\"--decay_factor\",                   type=float,     default=0.98,   help=\"Learning Rate-How much we decay.\")\n",
    "    parser.add_argument(\"--num_train_steps\",                type=int,       default=10000,  help=\"Num steps to train.\")\n",
    "    parser.add_argument(\"--colocate_gradients_with_ops\",    type=\"bool\",    default=True,   nargs=\"?\", const=True, help=\"Whether try colocating gradients with corresponding op\")\n",
    "\n",
    "\n",
    "    # initializer\n",
    "    parser.add_argument(\"--init_op\",        type=str,       default=\"glorot_normal\",    help=\"uniform | glorot_normal | glorot_uniform\")\n",
    "    parser.add_argument(\"--init_weight\",    type=float,     default=0.1,                help=\"for uniform init_op, initialize weights between [-this, this].\")\n",
    "\n",
    "    # data\n",
    "    parser.add_argument(\"--src\",            type=str,   default=None,   help=\"Source suffix, e.g., en.\")\n",
    "    parser.add_argument(\"--tgt\",            type=str,   default=None,   help=\"Target suffix, e.g., de.\")\n",
    "    parser.add_argument(\"--train_prefix\",   type=str,   default=None,   help=\"Train prefix, expect files with src/tgt suffixes.\")\n",
    "    parser.add_argument(\"--dev_prefix\",     type=str,   default=None,   help=\"Dev prefix, expect files with src/tgt suffixes.\")\n",
    "    parser.add_argument(\"--test_prefix\",    type=str,   default=None,   help=\"Test prefix, expect files with src/tgt suffixes.\")\n",
    "    parser.add_argument(\"--out_dir\",        type=str,   default=None,   help=\"Store log/model files.\")\n",
    "\n",
    "    # Vocab\n",
    "    parser.add_argument(\"--vocab_prefix\",   type=str,       default=None,       help=\"\"\"\\ Vocab prefix, expect files with src/tgt suffixes.If None, extract from train files.\\ \"\"\")\n",
    "    parser.add_argument(\"--sos\",            type=str,       default=\"<s>\",      help=\"Start-of-sentence symbol.\")\n",
    "    parser.add_argument(\"--eos\",            type=str,       default=\"</s>\",     help=\"End-of-sentence symbol.\")\n",
    "\n",
    "    # Sequence lengths\n",
    "    parser.add_argument(\"--src_max_len\",        type=int,   default=300,    help=\"Max length of src sequences during training.\")\n",
    "    parser.add_argument(\"--tgt_max_len\",        type=int,   default=50,     help=\"Max length of tgt sequences during training.\")\n",
    "    parser.add_argument(\"--src_max_len_infer\",  type=int,   default=300,    help=\"Max length of src sequences during inference.\")\n",
    "    parser.add_argument(\"--tgt_max_len_infer\",  type=int,   default=None,   help=\"\"\"\\ Max length of tgt sequences during inference. Also use to restrict the maximum decoding length.\\ \"\"\")\n",
    "\n",
    "    # Default settings works well (rarely need to change)\n",
    "    parser.add_argument(\"--unit_type\",          type=str,       default=\"lstm\", help=\"lstm | gru | layer_norm_lstm\")\n",
    "    parser.add_argument(\"--forget_bias\",        type=float,     default=1.0,    help=\"Forget bias for BasicLSTMCell.\")\n",
    "    parser.add_argument(\"--dropout\",            type=float,     default=0.2,    help=\"Dropout rate (not keep_prob)\")\n",
    "    parser.add_argument(\"--max_gradient_norm\",  type=float,     default=5.0,    help=\"Clip gradients to this norm.\")\n",
    "    parser.add_argument(\"--batch_size\",         type=int,       default=1,      help=\"Batch size.\")\n",
    "    parser.add_argument(\"--source_reverse\",     type=\"bool\",    default=False,  nargs=\"?\", const=True,  help=\"Reverse source sequence.\")\n",
    "\n",
    "    parser.add_argument(\"--eval_on_fly\",        type=\"bool\",    default=True,   help=\"Evaluate on Fly or save models for later evaluation\")\n",
    "    parser.add_argument(\"--snapshot_interval\",  type=int,       default=1000,   help=\"How often save snapshots while not evaluating on the fly\")\n",
    "    parser.add_argument(\"--steps_per_stats\",    type=int,       default=100,    help=\"How many training steps to do per stats logging.Save checkpoint every 10x steps_per_stats\")\n",
    "    parser.add_argument(\"--max_train\",          type=int,       default=0,      help=\"Limit on the size of training data (0: no limit).\")\n",
    "    parser.add_argument(\"--num_buckets\",        type=int,       default=0,      help=\"Put data into similar-length buckets.\")\n",
    "\n",
    "    # BPE\n",
    "    parser.add_argument(\"--bpe_delimiter\",  type=str,   default=None, help=\"Set to @@ to activate BPE\")\n",
    "\n",
    "    # Misc\n",
    "    parser.add_argument(\"--base_gpu\",                   type=int,       default=0,      help=\"ID of the GPU to start allocating from.\")\n",
    "    parser.add_argument(\"--num_gpus\",                   type=int,       default=1,      help=\"Number of gpus in each worker.\")\n",
    "    parser.add_argument(\"--log_device_placement\",       type=\"bool\",    default=False,  nargs=\"?\", const=True,  help=\"Debug GPU allocation.\")\n",
    "    parser.add_argument(\"--metrics\",                    type=str,       default=\"bleu\", help=\"Comma-separated list of evaluations metrics (bleu,rouge,accuracy)\")\n",
    "    parser.add_argument(\"--steps_per_external_eval\",    type=int,       default=None,   help=\"\"\"\\ How many training steps to do per external evaluation.  Automatically set based on data if None.\\ \"\"\")\n",
    "    parser.add_argument(\"--scope\",                      type=str,       default=None,   help=\"scope to put variables under\")\n",
    "    parser.add_argument(\"--hparams_path\",               type=str,       default=None,   help=\"Path to standard hparams json file that overrides hparams values from FLAGS.\")\n",
    "    parser.add_argument(\"--random_seed\",                type=int,       default=285,    help=\"Random seed (>0, set a specific seed).\")\n",
    "\n",
    "    # Inference\n",
    "    parser.add_argument(\"--ckpt\",                   type=str,   default=\"\",     help=\"Checkpoint file to load a model for inference.\")\n",
    "    parser.add_argument(\"--inference_input_file\",   type=str,   default=None,   help=\"Set to the text to decode.\")\n",
    "    parser.add_argument(\"--inference_list\",         type=str,   default=None,   help=\"A comma-separated list of sentence indices (0-based) to decode.\")\n",
    "    parser.add_argument(\"--infer_batch_size\",       type=int,   default=32,     help=\"Batch size for inference mode.\")\n",
    "    parser.add_argument(\"--inference_output_file\",  type=str,   default=None,   help=\"Output file to store decoding results.\")\n",
    "    parser.add_argument(\"--inference_ref_file\",     type=str,   default=None,   help=\"\"\"\\ Reference file to compute evaluation scores (if provided).\\ \"\"\")\n",
    "    parser.add_argument(\"--beam_width\",             type=int,   default=3,      help=\"\"\"\\ beam width when using beam search decoder. If 0 (default), use standard decoder with greedy helper.\\ \"\"\")\n",
    "    parser.add_argument(\"--length_penalty_weight\",  type=float, default=0.0,    help=\"Length penalty for beam search.\")\n",
    "\n",
    "    # Job info\n",
    "    parser.add_argument(\"--jobid\",          type=int, default=0, help=\"Task id of the worker.\")\n",
    "    parser.add_argument(\"--num_workers\",    type=int, default=1, help=\"Number of workers (inference only).\")\n",
    "\n",
    "\n",
    "def create_hparams(flags):\n",
    "    \"\"\"Create training hparams.\"\"\"\n",
    "    return tf.contrib.training.HParams(\n",
    "        # Data\n",
    "        src=flags.src,\n",
    "        tgt=flags.tgt,\n",
    "        train_prefix=flags.train_prefix,\n",
    "        dev_prefix=flags.dev_prefix,\n",
    "        test_prefix=flags.test_prefix,\n",
    "        vocab_prefix=flags.vocab_prefix,\n",
    "        out_dir=flags.out_dir,\n",
    "\n",
    "        # Networks\n",
    "        num_units=flags.num_units,\n",
    "        num_layers=flags.num_layers,\n",
    "        dropout=flags.dropout,\n",
    "        unit_type=flags.unit_type,\n",
    "        encoder_type=flags.encoder_type,\n",
    "        residual=flags.residual,\n",
    "        time_major=flags.time_major,\n",
    "        num_embeddings_partitions=flags.num_embeddings_partitions,\n",
    "\n",
    "        # Attention mechanisms\n",
    "        attention=flags.attention,\n",
    "        attention_architecture=flags.attention_architecture,\n",
    "        pass_hidden_state=flags.pass_hidden_state,\n",
    "\n",
    "        # Train\n",
    "        optimizer=flags.optimizer,\n",
    "        num_train_steps=flags.num_train_steps,\n",
    "        batch_size=flags.batch_size,\n",
    "        init_op=flags.init_op,\n",
    "        init_weight=flags.init_weight,\n",
    "        max_gradient_norm=flags.max_gradient_norm,\n",
    "        learning_rate=flags.learning_rate,\n",
    "        start_decay_step=flags.start_decay_step,\n",
    "        decay_factor=flags.decay_factor,\n",
    "        decay_steps=flags.decay_steps,\n",
    "        colocate_gradients_with_ops=flags.colocate_gradients_with_ops,\n",
    "\n",
    "        # Data constraints\n",
    "        num_buckets=flags.num_buckets,\n",
    "        max_train=flags.max_train,\n",
    "        src_max_len=flags.src_max_len,\n",
    "        tgt_max_len=flags.tgt_max_len,\n",
    "        source_reverse=flags.source_reverse,\n",
    "\n",
    "        # Inference\n",
    "        src_max_len_infer=flags.src_max_len_infer,\n",
    "        tgt_max_len_infer=flags.tgt_max_len_infer,\n",
    "        infer_batch_size=flags.infer_batch_size,\n",
    "        beam_width=flags.beam_width,\n",
    "        length_penalty_weight=flags.length_penalty_weight,\n",
    "\n",
    "        # Vocab\n",
    "        sos=flags.sos if flags.sos else vocab_utils.SOS,\n",
    "        eos=flags.eos if flags.eos else vocab_utils.EOS,\n",
    "        bpe_delimiter=flags.bpe_delimiter,\n",
    "\n",
    "        # Misc\n",
    "        base_gpu=flags.base_gpu,\n",
    "        forget_bias=flags.forget_bias,\n",
    "        num_gpus=flags.num_gpus,\n",
    "        epoch_step=0,  # record where we were within an epoch.\n",
    "        steps_per_stats=flags.steps_per_stats,\n",
    "        steps_per_external_eval=flags.steps_per_external_eval,\n",
    "        metrics=flags.metrics.split(\",\"),\n",
    "        log_device_placement=flags.log_device_placement,\n",
    "        random_seed=flags.random_seed,\n",
    "\n",
    "        eval_on_fly=flags.eval_on_fly,\n",
    "        snapshot_interval=flags.snapshot_interval,\n",
    "    )\n",
    "\n",
    "\n",
    "def extend_hparams(hparams):\n",
    "    \"\"\"Extend training hparams.\"\"\"\n",
    "    # Sanity checks\n",
    "    if hparams.encoder_type == \"bi\" and hparams.num_layers % 2 != 0:\n",
    "        raise ValueError(\"For bi, num_layers %d should be even\" % hparams.num_layers)\n",
    "\n",
    "    if hparams.attention_architecture in [\"gnmt\"] and hparams.num_layers < 2:\n",
    "        raise ValueError(\"For gnmt attention architecture, num_layers %d should be >= 2\" % hparams.num_layers)\n",
    "\n",
    "    # Flags\n",
    "    utils.print_out(\"# hparams:\")\n",
    "    utils.print_out(\"  src=%s\" % hparams.src)\n",
    "    utils.print_out(\"  tgt=%s\" % hparams.tgt)\n",
    "    utils.print_out(\"  train_prefix=%s\" % hparams.train_prefix)\n",
    "    utils.print_out(\"  dev_prefix=%s\" % hparams.dev_prefix)\n",
    "    utils.print_out(\"  test_prefix=%s\" % hparams.test_prefix)\n",
    "    utils.print_out(\"  out_dir=%s\" % hparams.out_dir)\n",
    "\n",
    "    # Set num_residual_layers\n",
    "    if hparams.residual and hparams.num_layers > 1:\n",
    "        if hparams.encoder_type == \"gnmt\":\n",
    "            # The first unidirectional layer (after the bi-directional layer) in\n",
    "            # the GNMT encoder can't have residual connection due to the input is\n",
    "            # the concatenation of fw_cell and bw_cell's outputs.\n",
    "            num_residual_layers = hparams.num_layers - 2\n",
    "        else:\n",
    "            num_residual_layers = hparams.num_layers - 1\n",
    "    else:\n",
    "        num_residual_layers = 0\n",
    "    hparams.add_hparam(\"num_residual_layers\", num_residual_layers)\n",
    "\n",
    "    ## Vocab\n",
    "    # Get vocab file names first\n",
    "    if hparams.vocab_prefix:\n",
    "        tgt_vocab_file = hparams.vocab_prefix + \".\" + hparams.tgt\n",
    "    else:\n",
    "        raise ValueError(\"hparams.vocab_prefix must be provided.\")\n",
    "\n",
    "    # Target Vocab\n",
    "    tgt_vocab_size, tgt_vocab_file = vocab_utils.check_vocab(tgt_vocab_file,\n",
    "                                                             hparams.out_dir,\n",
    "                                                             sos=hparams.sos,\n",
    "                                                             eos=hparams.eos,\n",
    "                                                             unk=vocab_utils.UNK)\n",
    "    hparams.add_hparam(\"tgt_vocab_size\", tgt_vocab_size)\n",
    "    hparams.add_hparam(\"tgt_vocab_file\", tgt_vocab_file)\n",
    "\n",
    "    # Check out_dir\n",
    "    if not tf.gfile.Exists(hparams.out_dir):\n",
    "        utils.print_out(\"# Creating output directory %s ...\" % hparams.out_dir)\n",
    "        tf.gfile.MakeDirs(hparams.out_dir)\n",
    "\n",
    "    # Evaluation\n",
    "    for metric in hparams.metrics:\n",
    "        hparams.add_hparam(\"best_\" + metric, 0)  # larger is better\n",
    "        best_metric_dir = os.path.join(hparams.out_dir, \"best_\" + metric)\n",
    "        hparams.add_hparam(\"best_bleu_dir\", best_metric_dir)\n",
    "        tf.gfile.MakeDirs(best_metric_dir)\n",
    "\n",
    "    return hparams\n",
    "\n",
    "\n",
    "def ensure_compatible_hparams(hparams, default_hparams, hparams_path):\n",
    "    \"\"\"Make sure the loaded hparams is compatible with new changes.\"\"\"\n",
    "\n",
    "    default_hparams = utils.maybe_parse_standard_hparams(default_hparams, hparams_path)\n",
    "\n",
    "    # For compatible reason, if there are new fields in default_hparams,\n",
    "    #   we add them to the current hparams\n",
    "    default_config = default_hparams.values()\n",
    "    config = hparams.values()\n",
    "    for key in default_config:\n",
    "        if key not in config:\n",
    "            hparams.add_hparam(key, default_config[key])\n",
    "\n",
    "    # Make sure that the loaded model has latest values for the below keys\n",
    "    updated_keys = [\"out_dir\", \"eval_on_fly\", \"snapshot_interval\" , \"base_gpu\", \"num_gpus\", \"test_prefix\", \"beam_width\", \"length_penalty_weight\", \"num_train_steps\"]\n",
    "\n",
    "    for key in updated_keys:\n",
    "        if key in default_config and getattr(hparams, key) != default_config[key]:\n",
    "            utils.print_out(\"# Updating hparams.%s: %s -> %s\" %\n",
    "                            (key, str(getattr(hparams, key)),\n",
    "                             str(default_config[key])))\n",
    "            setattr(hparams, key, default_config[key])\n",
    "\n",
    "    return hparams\n",
    "\n",
    "\n",
    "def create_or_load_hparams(out_dir, default_hparams, hparams_path):\n",
    "    \"\"\"Create hparams or load hparams from out_dir.\"\"\"\n",
    "    hparams = utils.load_hparams(out_dir)\n",
    "    if not hparams:\n",
    "        hparams = default_hparams\n",
    "        hparams = utils.maybe_parse_standard_hparams(\n",
    "            hparams, hparams_path)\n",
    "        hparams = extend_hparams(hparams)\n",
    "    else:\n",
    "        hparams = ensure_compatible_hparams(hparams, default_hparams, hparams_path)\n",
    "\n",
    "    # Save HParams\n",
    "    utils.save_hparams(out_dir, hparams)\n",
    "\n",
    "    for metric in hparams.metrics:\n",
    "        utils.save_hparams(getattr(hparams, \"best_bleu_dir\"), hparams)\n",
    "\n",
    "    # Print HParams\n",
    "    utils.print_hparams(hparams)\n",
    "    return hparams\n",
    "\n",
    "\n",
    "def run_main(flags, default_hparams, train_fn, inference_fn, target_session=\"\"):\n",
    "    \"\"\"Run main.\"\"\"\n",
    "    # Job\n",
    "    jobid = flags.jobid\n",
    "    num_workers = flags.num_workers\n",
    "    utils.print_out(\"# Job id %d\" % jobid)\n",
    "\n",
    "    # Random\n",
    "    random_seed = flags.random_seed\n",
    "    if random_seed is not None and random_seed > 0:\n",
    "        utils.print_out(\"# Set random seed to %d\" % random_seed)\n",
    "        random.seed(random_seed + jobid)\n",
    "        np.random.seed(random_seed + jobid)\n",
    "\n",
    "    ## Train / Decode\n",
    "    out_dir = flags.out_dir\n",
    "    if not tf.gfile.Exists(out_dir):\n",
    "        tf.gfile.MakeDirs(out_dir)\n",
    "\n",
    "    # Load hparams.\n",
    "    hparams = create_or_load_hparams(out_dir, default_hparams, flags.hparams_path)\n",
    "\n",
    "\n",
    "def main(unused_argv):\n",
    "    default_hparams = create_hparams(FLAGS)\n",
    "    train_fn = train.train\n",
    "    inference_fn = inference.inference\n",
    "    run_main(FLAGS, default_hparams, train_fn, inference_fn)\n",
    "    \n",
    "if __name__ == \"__main__\": #본 파일에서 실행될 때만 실행되도록 함\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    \n",
    "    nmt_parser = argparse.ArgumentParser()\n",
    "    add_arguments(nmt_parser)\n",
    "    FLAGS, unparsed = nmt_parser.parse_known_args()\n",
    "    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n",
    "    \n",
    "    w = Target()\n",
    "    w.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
